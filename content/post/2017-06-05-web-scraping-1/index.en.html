---
title: 'How Web Scraping eases my job searching pain? - Part I : Scrape contents from
  one URL'
author: Yin-Ting
date: '2017-06-05'
slug: web-scraping-1
aliases: 
  - /posts/web-scraping-1
categories:
  - Python
  - Web Scraping  
tags:
draft: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p><span style="color: darkred">
<strong>Update : 2018-05-06</strong> <br />
I have found that the website structure of <a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a>, <a href="http://www.dice.com/">Dice</a>, <a href="http://www.careerbuilder.com/">Careerbuilder</a> have been changed since I wrote these posts. So, some of my code may not work now. But, I think the concept and the process is still the same.
</span></p>
<hr />
<p><strong>The Github repository for this project :</strong> <a href="https://github.com/choux130/webscraping_example">choux130/webscraping_example</a>.</p>
<p>The process of finding jobs online can definitely be a torture. Every day is started by sitting in front of computer, browsing different kind of job searching websites and trying to track every job I have read and then categorizing them into interested or not interested. The whole process is mostly about clicking on different pages and websites, copying and pasting words. At first, you may have enough patience to read and track about 50 or more jobs a day, but when time gets long, you start to feel depressed and feel like yourself is just like a robot. If you have this same kind of experience, you are welcom to check out this post and see how I use automated web scraping techniques in <a href="https://www.python.org/downloads/release/python-360/">Python 3.6.0</a> to make my job searching process easier, more efficient and much more fun! <br />
<br />
<a href="/posts/2017-06-05-web-scraping-1/web_jobs.png">
<img src="/posts/2017-06-05-web-scraping-1/web_jobs.png" style="width:100%"/></a>
<br />
This is just a quick look for one of my final output which was automatically generated by running the Python code. The good thing for this sheet is that I can have a big overview for all the job searching results with their required skills, education level, major and self-interested key words. So, I can easily prioritize the jobs and only focus on ones I think I can be a good fit. Compared to clicking all the job link one by one on all the job searching websites, it really not only saves me a lot of time for the first screening but also make me healthier in mental which to me is the most important thing! If you also think this sheet can be a good idea for your job searching process, please keep reading and grab anything worthy to you.</p>
<p>If you are not a Python 3.6.0 user or only know little about it, don’t worry! Before I did this project, I also know nothing about Python and I just learned by doing. You can find all the example code in my <a href="https://github.com/choux130/webscraping_example">Github repo - webscraping_example</a>. My code may not be the most perfect but it works well on me.</p>
<div id="details" class="section level3">
<h3>Details</h3>
<ul>
<li><strong><font size="4">References</font></strong> <br />
Thank you all so much!
<ul>
<li><a href="https://github.com/JianhuaHuang/Data_Scientist_Skills_Python_R">JianhuaHuang - Data_Scientist_Skills_Python_R</a></li>
<li><a href="http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/">Greg Reda - Web Scraping 101 with Python</a></li>
<li><a href="https://blog.nycdatascience.com/student-works/project-3-web-scraping-company-data-from-indeed-com-and-dice-com/">Sung Pil Moon - Web Scraping company data from Indeed.com and Dice.com</a></li>
<li><a href="https://blog.nycdatascience.com/student-works/web-scraping/glassdoor-web-scraping/">Diego De Lazzari - Landing my dream job by scraping Glassdoor.com</a></li>
<li><a href="https://www.crummy.com/software/BeautifulSoup/bs4/doc/">Beautiful Soup Documentation</a></li>
</ul></li>
<li><strong><font size="4">Goals</font></strong> <br />
<ol style="list-style-type: decimal">
<li>Automatically generate a sheet that can include today’s all searching results from 4 different job searching websites (<a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a>, <a href="http://www.dice.com/">Dice</a>, <a href="http://www.careerbuilder.com/">Careerbuilder</a>).</li>
<li>For each job result, I would like to know the following things: <br />
<ul>
<li>From which Job Searching Website</li>
<li>Job ID (related to Job Link)</li>
<li>Job Title</li>
<li>Job Company</li>
<li>Location</li>
<li>Job Link</li>
<li>Job Type</li>
<li>Required Skills</li>
<li>Required Education Level</li>
<li>Preferred Majors</li>
<li>Interesting Keywords</li>
<li>All the Text in the Page</li>
</ul></li>
</ol></li>
<li><strong><font size="4">Steps</font></strong> <br />
For all the following steps, I will use web scraping <a href="http://www.careerbuilder.com/">Careerbuilder</a> as an example. If you are interested in how to do it on other job searching websites ( <a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a> and <a href="http://www.dice.com/">Dice</a>), please check out my <a href="https://github.com/choux130/webscraping_example">Github repo - webscraping_example</a>. <br />
Example searching criteria:
<ul>
<li>job searching website: <i><u>Careerbuilder</u></i></li>
<li>searching keywords: <i><u>Data Scientist</u></i></li>
<li>city: <i><u>Durham</u></i></li>
<li>state: <i><u>NC</u></i></li>
</ul>
<ol style="list-style-type: decimal">
<li><p><strong>Know the URL of the webpage you are going to scrape and know how they related to your searching criteria.</strong> <br />
<span>
The URL for our example,
<a href="http://www.careerbuilder.com/jobs-data-scientist-in-durham,nc">http://www.careerbuilder.com/jobs-data-scientist-in-durham,nc</a>. </span>
<span></p>
<pre class="python"><code>########################################################
#################### IMPORT LIBRARY ####################
########################################################
import bs4
import numpy
import pandas
import re
import requests
import datetime
import stop_words

###################################################
#################### ARGUMENTS ####################
###################################################
input_job = &quot;data scientist&quot;
input_quote = False # add &quot;&quot; on your searching keywords
input_city = &quot;Durham&quot;
input_state = &quot;NC&quot;
sign = &quot;-&quot;
BASE_URL_careerbuilder = &#39;http://www.careerbuilder.com&#39;

#####################################################
##### Function for Transform searching keywords #####
#####################################################
# The default &quot;quote = False&quot;
def transform(input,sign, quote = False):
  syntax = input.replace(&quot; &quot;, sign)
  if quote == True:
    syntax = &#39;&#39;.join([&#39;%2522&#39;, syntax, &#39;%2522&#39;])
  return(syntax)

######################################
########## Generate the URL ##########
######################################
if not input_city: # if (input_city is &quot;&quot;)
    url_dice_list = [ BASE_URL_dice, &#39;/jobs?q=&#39;, transform(input_job, sign , input_quote), &#39;+&amp;l=&#39;, input_state ]
    url_dice = &#39;&#39;.join(url_dice_list)
else: # input_city is not &quot;&quot;
    url_dice_list = [ BASE_URL_dice, &#39;/jobs?q=&#39;, transform(input_job, sign , input_quote), &#39;&amp;l=&#39;, transform(input_city, sign), &#39;%2C+&#39;, input_state ]
    url_dice = &#39;&#39;.join(url_dice_list)
print(url_dice)</code></pre>
<p><span><code class = "hljs shell">
<code>https://www.dice.com/jobs?q=Data+Scientist&amp;l=Durham%2C+NC</code>
</code></span></p></li>
<li><p><strong>Scrape general contents from the webpage.</strong> <br />
To catch the important info from the webpage, we have to start from its HTML code. For the chrome, click right mouse button and choose <em>Inspect</em>. And for the Safari, do the same thing but choose <em>Inspect Element</em>. Then now, we can see all the HTML code for the webpage.</p>
<ul>
<li><p><strong>Total Number of the results</strong>
<a href="/posts/2017-06-05-web-scraping-1/job_num.png"><img src="/posts/2017-06-05-web-scraping-1/job_num.png" style="width:100%"></a></p>
<pre class="python"><code># get the HTML code from the URL
rawcode_careerbuilder = requests.get(url_careerbuilder)

# Choose &quot;lxml&quot; as parser
soup_careerbuilder = bs4.BeautifulSoup(rawcode_careerbuilder.text, &quot;lxml&quot;)

# total number of results
num_total_careerbuilder = soup_careerbuilder.find(&#39;div&#39;, {&#39;class&#39; : &#39;count&#39;}).contents[0]
print(num_total_careerbuilder)
num_total_careerbuilder = int(re.sub(&#39;[\(\)\{\}&lt;&gt;]&#39;, &#39;&#39;,
                  num_total_careerbuilder).split()[0])
print(num_total_careerbuilder)</code></pre>
<pre><code class="language-text" data-lang="text"><span>(26 Jobs)
26</span></code></pre></li>
<li><p><strong>Total Number of pages</strong> <br />
<span>
By doing some test and observation, I know that there are 25 positions in one page. Then, I can calculate the total number of pages. </span></p>
<pre class="python"><code># total number of pages
num_pages_careerbuilder = int(numpy.ceil(num_total_careerbuilder/25.0))
print(num_pages_careerbuilder)</code></pre>
<pre><code class="language-text" data-lang="text"><span>2</span></code></pre></li>
</ul></li>
<li><p><strong>Scrape all positions with their basic info for each page.</strong> <br /></p>
<ul>
<li><p><strong>The basic info for each position is in its <code>&lt;div class="job-row"&gt;...&lt;/div&gt;</code> chunk.</strong> <span>So, in our code, the first steps would be picking out all the <code>&lt;div class="job-row"&gt;...&lt;/div&gt;</code> chunks. <br /><br /></span>
<a href="/posts/2017-06-05-web-scraping-1/job.png"><img src="/posts/2017-06-05-web-scraping-1/job.png" style="width:100%"></a> <br /></p></li>
<li><p><strong>For each chunk (or each position), we have following basic features.</strong></p>
<ul>
<li><i>Job Title</i>, <i>Job ID</i> and <i>Job Link</i> <br />
<a href="/posts/2017-06-05-web-scraping-1/job_title.png"><img src="/posts/2017-06-05-web-scraping-1/job_title.png" style="width:100%"></a> <br /></li>
<li><i>Job Company</i> <br />
<a href="/posts/2017-06-05-web-scraping-1/job_company.png"><img src="/posts/2017-06-05-web-scraping-1/job_company.png" style="width:100%"></a> <br /></li>
<li><i>Location</i> <br />
<a href="/posts/2017-06-05-web-scraping-1/job_location.png"><img src="/posts/2017-06-05-web-scraping-1/job_location.png" style="width:100%"></a></li>
</ul></li>
<li><p><strong>The code for picking out all the basic info for each job.</strong></p>
<pre><code class="python"><span># create an empty dataframe
job_df_careerbuilder = pandas.DataFrame()

# the date for today
now = datetime.datetime.now()
now_str = now.strftime("%m/%d/%Y")
now_str_name=now.strftime('%m%d%Y')

########################################
##### Loop for all the total pages #####
########################################
for i in range(1, num_pages_careerbuilder+1):
    # generate the URL
    url = ''.join([url_careerbuilder,'?page_number=', str(i)])
    print(url)

    # get the HTML code from the URL
    rawcode = requests.get(url)
    soup = bs4.BeautifulSoup(rawcode.text, "lxml")

    # pick out all the "div" with "class="job-row"
    divs = soup.findAll("div")
    job_divs = [jp for jp in divs if not jp.get('class') is None
                    and 'job-row' in jp.get('class')]

    # loop for each div chunk
    for job in job_divs:
        try:
            # job id
            id = job.find('h2',{'class' : 'job-title'}).find('a').attrs['data-job-did']
            # job link related to job id
            link = BASE_URL_careerbuilder + '/job/' + id
            # job title
            title = job.find('h2', {'class' : 'job-title'}).text.strip()
            # job company
          company = job.find('div', {'class' : 'columns large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
            # job location
          location = job.find('div', {'class' : 'columns end large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
        except:
            continue

        job_df_careerbuilder =  job_df_careerbuilder.append({'job_title': title,
                      'job_id': id,
                      'job_company': company,
                      'date': now_str,
                      'from':'Careerbuilder',
                      'job_location':location,
                      'job_link':link},ignore_index=True)</span></code></pre>
<pre><code class="language-text" data-lang="text"><span>http://www.careerbuilder.com/jobs-Data-Scientist-in-Durham,NC?page_number=1
http://www.careerbuilder.com/jobs-Data-Scientist-in-Durham,NC?page_number=2</span></code></pre></li>
<li><p><strong>Save the dataframe</strong></p>
<pre class="python"><code># reorder the columns of dataframe
cols=[&#39;from&#39;,&#39;date&#39;,&#39;job_id&#39;,&#39;job_title&#39;,&#39;job_company&#39;,&#39;job_location&#39;,&#39;job_link&#39;]
job_df_careerbuilder = job_df_careerbuilder[cols]

# delete the duplicated jobs using job link
job_df_careerbuilder = job_df_careerbuilder.drop_duplicates([&#39;job_link&#39;], keep=&#39;first&#39;)

# print the dimenstion of the dataframe
print(job_df_careerbuilder.shape)

# save the data frame as a csv file
path = &#39;/path/output/&#39; + &#39;job_careerbuilder_&#39; + now_str_name + &#39;.csv&#39;
job_df_careerbuilder.to_csv(path)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;</code></pre>
<p><span><code>(26, 7)</code></span>
<br /><br />
<a href="/posts/2017-06-05-web-scraping-1/web_jobs2.png">
<img src="/posts/2017-06-05-web-scraping-1/web_jobs2.png" style="width:100%"/></a></p></li>
</ul></li>
</ol></li>
<li><strong><font size="4">Only Part I is done! Don’t miss Part II! </font></strong> <br />
Yes, the data frame we have now is not complete yet. Now, we only list out all the searching results in a nice <code>.csv</code> but this is not enough for helping us efficiently find jobs. So, for the next post I will talk about how to scrape all the URLs in our <code>.csv</code> file and then to see if they have any contents or words we are interested in. This is the next post, <strong><a href="/posts/web-scraping-2/">How Web Scraping eases my job searching pain? - Part II : Scrape contents from a list of URLs</a></strong>. Again, all the example python code can be found in my <a href="https://github.com/choux130/webscraping_example">Github repo - webscraping_example</a>.</li>
</ul>
</div>
