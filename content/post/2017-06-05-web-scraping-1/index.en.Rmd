---
title: 'How Web Scraping eases my job searching pain? - Part I : Scrape contents from
  one URL'
author: Yin-Ting
date: '2017-06-05'
slug: web-scraping-1
aliases: 
  - /posts/web-scraping-1
categories:
  - Python
  - Web Scraping
tags:
draft: no
---

<span style="color: darkred">
**Update : 2018-05-06** <br /> 
I have found that the website structure of <a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a>, <a href="http://www.dice.com/">Dice</a>, <a href="http://www.careerbuilder.com/">Careerbuilder</a> have been changed since I wrote these posts. So, some of my code may not work now. But, I think the concept and the process is still the same. 
</span> 

<hr />

**The Github repository for this project :** [choux130/webscraping_example](https://github.com/choux130/webscraping_example).

The process of finding jobs online can definitely be a torture. Every day is started by sitting in front of computer, browsing different kind of job searching websites and trying to track every job I have read and then categorizing them into interested or not interested. The whole process is mostly about clicking on different pages and websites, copying and pasting words. At first, you may have enough patience to read and track about 50 or more jobs a day, but when time gets long, you start to feel depressed and feel like yourself is just like a robot. If you have this same kind of experience, you are welcom to check out this post and see how I use automated web scraping techniques in [Python 3.6.0](https://www.python.org/downloads/release/python-360/) to make my job searching process easier, more efficient and much more fun! <br />
<br />
<a href="/posts/2017-06-05-web-scraping-1/web_jobs.png">
<img src="/posts/2017-06-05-web-scraping-1/web_jobs.png" style="width:100%"/></a>
<br />
This is just a quick look for one of my final output which was automatically generated by running the Python code. The good thing for this sheet is that I can have a big overview for all the job searching results with their required skills, education level, major and self-interested key words. So, I can easily prioritize the jobs and only focus on ones I think I can be a good fit. Compared to clicking all the job link one by one on all the job searching websites, it really not only saves me a lot of time for the first screening but also make me healthier in mental which to me is the most important thing! If you also think this sheet can be a good idea for your job searching process, please keep reading and grab anything worthy to you.

If you are not a Python 3.6.0 user or only know little about it, don't worry! Before I did this project, I also know nothing about Python and I just learned by doing. You can find all the example code in my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example). My code may not be the most perfect but it works well on me.

### Details
* **<font size="4">References</font>** <br />
  Thank you all so much!
  * [JianhuaHuang - Data_Scientist_Skills_Python_R](https://github.com/JianhuaHuang/Data_Scientist_Skills_Python_R)
  * [Greg Reda - Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/)
  * [Sung Pil Moon - Web Scraping company data from Indeed.com and Dice.com](https://blog.nycdatascience.com/student-works/project-3-web-scraping-company-data-from-indeed-com-and-dice-com/)
  * [Diego De Lazzari - Landing my dream job by scraping Glassdoor.com](https://blog.nycdatascience.com/student-works/web-scraping/glassdoor-web-scraping/)
  * [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)

* **<font size="4">Goals</font>** <br />
  1. Automatically generate a sheet that can include today's all searching results from 4 different job searching websites (<a href="https://www.indeed.com/">Indeed</a>, <a href="https://www.monster.com/">Monster</a>, <a href="http://www.dice.com/">Dice</a>, <a href="http://www.careerbuilder.com/">Careerbuilder</a>).
  2. For each job result, I would like to know the following things: <br />
      * From which Job Searching Website
      * Job ID (related to Job Link)
      * Job Title
      * Job Company
      * Location
      * Job Link
      * Job Type
      * Required Skills
      * Required Education Level
      * Preferred Majors
      * Interesting Keywords
      * All the Text in the Page

* **<font size="4">Steps</font>** <br />
    For all the following steps, I will use web scraping [Careerbuilder](http://www.careerbuilder.com/) as an example. If you are interested in how to do it on other job searching websites ( [Indeed](https://www.indeed.com/), [Monster](https://www.monster.com/) and [Dice](http://www.dice.com/)), please check out my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example). <br />
  Example searching criteria:
    - job searching website: <i><u>Careerbuilder</u></i>
    - searching keywords: <i><u>Data Scientist</u></i>
    - city: <i><u>Durham</u></i>
    - state: <i><u>NC</u></i>

    1. **Know the URL of the webpage you are going to scrape and know how they related to your searching criteria.** <br />
    <span>
    The URL for our example,
[http://www.careerbuilder.com/jobs-data-scientist-in-durham,nc](http://www.careerbuilder.com/jobs-data-scientist-in-durham,nc). </span>
<span>

        ``` python
        ########################################################
        #################### IMPORT LIBRARY ####################
        ########################################################
        import bs4
        import numpy
        import pandas
        import re
        import requests
        import datetime
        import stop_words

        ###################################################
        #################### ARGUMENTS ####################
        ###################################################
        input_job = "data scientist"
        input_quote = False # add "" on your searching keywords
        input_city = "Durham"
        input_state = "NC"
        sign = "-"
        BASE_URL_careerbuilder = 'http://www.careerbuilder.com'

        #####################################################
        ##### Function for Transform searching keywords #####
        #####################################################
        # The default "quote = False"
        def transform(input,sign, quote = False):
          syntax = input.replace(" ", sign)
          if quote == True:
            syntax = ''.join(['%2522', syntax, '%2522'])
          return(syntax)

        ######################################
        ########## Generate the URL ##########
        ######################################
        if not input_city: # if (input_city is "")
            url_dice_list = [ BASE_URL_dice, '/jobs?q=', transform(input_job, sign , input_quote), '+&l=', input_state ]
            url_dice = ''.join(url_dice_list)
        else: # input_city is not ""
            url_dice_list = [ BASE_URL_dice, '/jobs?q=', transform(input_job, sign , input_quote), '&l=', transform(input_city, sign), '%2C+', input_state ]
            url_dice = ''.join(url_dice_list)
        print(url_dice)
        ```
        <span><code class = "hljs shell">
        `https://www.dice.com/jobs?q=Data+Scientist&l=Durham%2C+NC`
</code></span>

    2. **Scrape general contents from the webpage.** <br />
      To catch the important info from the webpage, we have to start from its HTML code. For the chrome, click right mouse button and choose *Inspect*. And for the Safari, do the same thing but choose *Inspect Element*. Then now, we can see all the HTML code for the webpage. 
        * **Total Number of the results**
          <a href="/posts/2017-06-05-web-scraping-1/job_num.png"><img src="/posts/2017-06-05-web-scraping-1/job_num.png" style="width:100%"></a>
          
            ``` python
            # get the HTML code from the URL
            rawcode_careerbuilder = requests.get(url_careerbuilder)
  
            # Choose "lxml" as parser
            soup_careerbuilder = bs4.BeautifulSoup(rawcode_careerbuilder.text, "lxml")
  
            # total number of results
            num_total_careerbuilder = soup_careerbuilder.find('div', {'class' : 'count'}).contents[0]
            print(num_total_careerbuilder)
            num_total_careerbuilder = int(re.sub('[\(\)\{\}<>]', '',
                              num_total_careerbuilder).split()[0])
            print(num_total_careerbuilder)
            ```
          <pre><code class="language-text" data-lang="text"><span>(26 Jobs)
          26</span></code></pre>

        * **Total Number of pages** <br />
          <span>
          By doing some test and observation, I know that there are 25 positions in one page. Then, I can calculate the total number of pages. </span>
          ``` python
          # total number of pages
          num_pages_careerbuilder = int(numpy.ceil(num_total_careerbuilder/25.0))
          print(num_pages_careerbuilder)
          ```
          <pre><code class="language-text" data-lang="text"><span>2</span></code></pre>

    3. **Scrape all positions with their basic info for each page.** <br />
        * **The basic info for each position is in its `<div class="job-row">...</div>` chunk.** <span>So, in our code, the first steps would be picking out all the `<div class="job-row">...</div>` chunks. <br /><br /></span>
          <a href="/posts/2017-06-05-web-scraping-1/job.png"><img src="/posts/2017-06-05-web-scraping-1/job.png" style="width:100%"></a> <br />

        * **For each chunk (or each position), we have following basic features.**
           * <i>Job Title</i>, <i>Job ID</i> and <i>Job Link</i> <br />
          <a href="/posts/2017-06-05-web-scraping-1/job_title.png"><img src="/posts/2017-06-05-web-scraping-1/job_title.png" style="width:100%"></a> <br />
            * <i>Job Company</i> <br />
          <a href="/posts/2017-06-05-web-scraping-1/job_company.png"><img src="/posts/2017-06-05-web-scraping-1/job_company.png" style="width:100%"></a> <br />
            * <i>Location</i> <br />
          <a href="/posts/2017-06-05-web-scraping-1/job_location.png"><img src="/posts/2017-06-05-web-scraping-1/job_location.png" style="width:100%"></a>

        * **The code for picking out all the basic info for each job.**

            <pre><code class="python"><span># create an empty dataframe
          job_df_careerbuilder = pandas.DataFrame()
  
          # the date for today
          now = datetime.datetime.now()
          now_str = now.strftime("%m/%d/%Y")
          now_str_name=now.strftime('%m%d%Y')

          ########################################
          ##### Loop for all the total pages #####
          ########################################
          for i in range(1, num_pages_careerbuilder+1):
              # generate the URL
              url = ''.join([url_careerbuilder,'?page_number=', str(i)])
              print(url)

              # get the HTML code from the URL
              rawcode = requests.get(url)
              soup = bs4.BeautifulSoup(rawcode.text, "lxml")

              # pick out all the "div" with "class="job-row"
              divs = soup.findAll("div")
              job_divs = [jp for jp in divs if not jp.get('class') is None
                              and 'job-row' in jp.get('class')]

              # loop for each div chunk
              for job in job_divs:
                  try:
                      # job id
                      id = job.find('h2',{'class' : 'job-title'}).find('a').attrs['data-job-did']
                      # job link related to job id
                      link = BASE_URL_careerbuilder + '/job/' + id
                      # job title
                      title = job.find('h2', {'class' : 'job-title'}).text.strip()
                      # job company
                    company = job.find('div', {'class' : 'columns large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
                      # job location
                    location = job.find('div', {'class' : 'columns end large-2 medium-3 small-12'}).find('h4', {'class': 'job-text'}).text.strip()
                  except:
                      continue

                  job_df_careerbuilder =  job_df_careerbuilder.append({'job_title': title,
                                'job_id': id,
                                'job_company': company,
                                'date': now_str,
                                'from':'Careerbuilder',
                                'job_location':location,
                                'job_link':link},ignore_index=True)</span></code></pre>
          <pre><code class="language-text" data-lang="text"><span>http://www.careerbuilder.com/jobs-Data-Scientist-in-Durham,NC?page_number=1
          http://www.careerbuilder.com/jobs-Data-Scientist-in-Durham,NC?page_number=2</span></code></pre>
          
        * **Save the dataframe**
          
            ```python
            # reorder the columns of dataframe
            cols=['from','date','job_id','job_title','job_company','job_location','job_link']
            job_df_careerbuilder = job_df_careerbuilder[cols]
  
            # delete the duplicated jobs using job link
            job_df_careerbuilder = job_df_careerbuilder.drop_duplicates(['job_link'], keep='first')
  
            # print the dimenstion of the dataframe
            print(job_df_careerbuilder.shape)
  
            # save the data frame as a csv file
            path = '/path/output/' + 'job_careerbuilder_' + now_str_name + '.csv'
            job_df_careerbuilder.to_csv(path)</span></code></pre>
            ```
          
            <span>`(26, 7)`</span>
          <br /><br />
          <a href="/posts/2017-06-05-web-scraping-1/web_jobs2.png">
          <img src="/posts/2017-06-05-web-scraping-1/web_jobs2.png" style="width:100%"/></a>

* **<font size="4">Only Part I is done! Don't miss Part II! </font>** <br />
  Yes, the data frame we have now is not complete yet. Now, we only list out all the searching results in a nice `.csv` but this is not enough for helping us  efficiently find jobs. So, for the next post I will talk about how to scrape all the URLs in our `.csv` file and then to see if they have any contents or words we are interested in. This is the next post, **<a href="/posts/web-scraping-2/">How Web Scraping eases my job searching pain? - Part II : Scrape contents from a list of URLs</a>**. Again, all the example python code can be found in my [Github repo - webscraping_example](https://github.com/choux130/webscraping_example).

