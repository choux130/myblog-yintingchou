---
title: Naive Bayes Classifier
author: Yin-Ting
date: '2017-02-12'
slug: naive-bayes-classifier
aliases: 
  - /posts/naive-bayes-classifier
categories:
  - Methodology
tags:
  - Classification
draft: no
---

<script src="{{< blogdown/postref >}}index.en_files/header-attrs/header-attrs.js"></script>


<p>Naive Bayes Classifier is a simple and intuitive method for the classification. The algorithm is based on Bayes’ Theorem with two assumptions on predictors: conditionally independent and equal importance. This technique mainly works on categorical response and explanatory variables. But it still can work on numeric explanatory variables as long as it can be transformed to categorical variables.</p>
<p>This post is my note about Naive Bayes Classifier, a classification teachniques. All the contents in this post are based on my reading on many resources which are listed in the References part.</p>
<div id="references" class="section level3">
<h3>References</h3>
<ul>
<li><strong>Books</strong>
<ol style="list-style-type: decimal">
<li><a href="https://statweb.stanford.edu/~tibs/ElemStatLearn/">The Elements of Statistical Learning:Data Mining, Inference, and Prediction</a></li>
<li><a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning with Applications in R</a></li>
<li><a href="http://shop.oreilly.com/product/9781784393908.do">Machine Learning with R</a></li>
</ol></li>
</ul>
</div>
<div id="details" class="section level3">
<h3>Details</h3>
<ul>
<li><strong><font size="4">Name</font></strong> : Naive Bayes Classifier
<br / >
<br / ></li>
<li><strong><font size="4">Data Type</font></strong> :
<ul>
<li><strong>Reponse Variable</strong> : Categorical <br /></li>
<li><strong>Explanatory Variable</strong> : Categorical and Numeric <br />
(The numeric variables need to be discretized by binning or using probability density function.) <br />
The below picture originated from : <a href="http://www.saedsayad.com/naive_bayesian.htm">here</a>
<img src="/posts/2017-02-12-naive-bayes-classifier/numeric.png" style="width:600px"/>
<br />
<br /></li>
</ul></li>
<li><strong><font size="4">Assumptions</font></strong> :
<ol style="list-style-type: decimal">
<li><strong>All the predictors have equal importance to the response variable.</strong> <br />
In other words, all predictors will be put in the algorithm even though some of them may not be as influential as others.</li>
<li><strong>All the predictors are conditionally independent to each other given in any class.</strong>
That means, we always treat them conditionally independent no matter it is true or not. For example, suppose we have two datasets. In the first dataset, <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span> given <span class="math inline">\(Type=A\)</span> is kind of independent to each other, but in the second datset, they are completely dependent to each other. <br /> <br />
<img src="/posts/2017-02-12-naive-bayes-classifier/table_nbc_3.png" style="width:500px"/>
<br />
Nevertheless, they all have they same contingency table and will have the same result of Naive Bayes Classifier. <br /><br />
<img src="/posts/2017-02-12-naive-bayes-classifier/table_nbc_1.png" style="width:500px"/>
<br />
<br /></li>
</ol></li>
<li><strong><font size="4"> Bayes’ Theorem</font></strong></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(A|B) &amp;= \frac{P(A \cap B)}{P(B)} \\[5pt]
&amp;= \frac{P(B|A)P(A)}{P(B)} \\[5pt]
&amp;= \frac{P(B|A)P(A)}{\sum_{i}^{}P(B|{A}_{i})P({A}_{i})}
\end{align}
\]</span></p>
<p><br /><br /></p>
<ul>
<li><p><strong><font size="4">Algorithm</font></strong> <br />
Given a class variable <span class="math display">\[Y= \{ 1, 2,..., K \}, K\geq2\]</span> and explanatory variables, <span class="math display">\[X=\{ X_1, X_2,..., X_p \}, \]</span> the Bayes’ Theorem can be written as:
<span class="math display">\[
\begin{align}
P(Y=k|X=x) &amp;= \frac{P(X=x|Y=k)P(Y=k)}{P(X=x)} \\[5pt]
&amp;= \frac{P(X=x|Y=k)P(Y=k)}{\sum_{i=1}^{K}P(X=x|Y=i)P(Y=i)}
\end{align}
\]</span></p>
<p>The Naive Bayes Classifier is a function, <span class="math display">\[C \colon \mathbb{R}^p \rightarrow \{ 1, 2,..., K \}\]</span> defined as</p>
<p><span class="math display">\[
  \begin{align}
  C(x) &amp;=\underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(Y=k|X=x) \\[5pt]
      &amp;= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(X=x|Y=k)P(Y=k) \\[5pt]
      \quad &amp;(\text{by assuming that } X_1,...,X_p \text{ are conditionally independent when given } Y=k, \forall k\in \{ 1, 2,..., K \}) \\[5pt]
      &amp;= \underset{k\in \{ 1, 2,..., K \} }{\operatorname{argmax}}P(X_1=x_1|Y=k)P(X_2=x_2|Y=k)\cdots P(X_p=x_p|Y=k)P(Y=k)
  \end{align}
  \]</span></p></li>
<li><p><strong><font size="4">Strengths and Weaknesses</font></strong> <br /></p>
<ul>
<li><strong>Strengths</strong>: <br />
<ol style="list-style-type: decimal">
<li>simple and effective</li>
</ol></li>
<li><strong>Weaknesses</strong>: <br />
<ol style="list-style-type: decimal">
<li>hard to meet the assumptions of eaqual important and mutual independence on predictors.</li>
<li>not good to deal with many numeric predictors.</li>
</ol></li>
</ul></li>
<li><p><strong><font size="4">A Simple Example</font></strong> <br />
Suppose we have a contingency table like this:</p>
<p><img src="/posts/2017-02-12-naive-bayes-classifier/table_nbc_2.png" style="width:500px"></p>
<p><strong>Q :</strong> And, what will be our guess on type if we have a data has X1=“Yes” and X2=“Unsure”? <br />
<strong>A :</strong> Our guess is Type B.</p></li>
</ul>
<p><span class="math display">\[
\begin{align}
P(A|X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}) &amp;\propto P(X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}|A)P(A) \\[5pt]
  &amp;= P(X_1=\text{&quot;Yes&quot;}|A)P(X_2=\text{&quot;Unsure&quot;}|A)P(A) \\[5pt]
  &amp;= \frac{10}{50} \cdot \frac{30}{50} \cdot \frac{50}{150} \\[5pt]
  &amp;= \frac{1}{25} \\[10pt]
P(B|X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}) &amp;\propto P(X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}|B)P(B) \\[5pt]
  &amp;= P(X_1=\text{&quot;Yes&quot;}|B)P(X_2=\text{&quot;Unsure&quot;}|B)P(Type=B) \\[5pt]
  &amp;= \frac{70}{100} \cdot \frac{10}{100} \cdot \frac{100}{150} \\[5pt]
  &amp;= \frac{14}{300} \\[10pt]
C(X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}) &amp;= \underset{k\in \{ A, B \} }{\operatorname{argmax}}P(Y=k|X_1=\text{&quot;Yes&quot;}, X_2=\text{&quot;Unsure&quot;}) \\[5pt]
      &amp;= B
\end{align}
\]</span></p>
<ul>
<li><strong><font size="4">Further topics</font></strong>
<ul>
<li><strong>Laplace Estimator</strong> (<a href="http://shop.oreilly.com/product/9781784393908.do">Machine Learning with R</a>, Chapter 4) <br />
Adding a small number to the frequency table to avoid zero probability for the Naive Bayes Classifier.</li>
</ul></li>
</ul>
</div>
